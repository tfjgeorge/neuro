{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 30)\n"
     ]
    }
   ],
   "source": [
    "from read_data import load_data_graphmetrics as load_data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import itertools\n",
    "\n",
    "d = load_data()\n",
    "\n",
    "fields = ['Clustering_coeff', 'Assortativity', 'Assortativity_weighted', 'Clustering_coeff_weighted', 'Global_efficiency']\n",
    "\n",
    "d_all = numpy.concatenate([d[freq][field][:, None]\n",
    "                           for freq, field in itertools.product(d.keys(), fields)], axis=1)\n",
    "\n",
    "print d_all.shape\n",
    "\n",
    "session = d['alpha']['Session']\n",
    "session_n = []\n",
    "\n",
    "for i in session:                                                                                \n",
    "    if i == 'rest':\n",
    "        session_n.append(0)\n",
    "    elif i == 'samatha':\n",
    "        session_n.append(1)\n",
    "    else:\n",
    "        session_n.append(2)\n",
    "        \n",
    "session_n = numpy.array(session_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor, function, shared, config, gradient\n",
    "\n",
    "print config.floatX\n",
    "\n",
    "class MLP():\n",
    "    \n",
    "    def __init__(self, layers=[30, 20, 20, 3]):\n",
    "        \n",
    "        self.parameters = []\n",
    "        self.X = tensor.fmatrix()\n",
    "        self.y = tensor.ivector()\n",
    "        \n",
    "        out = self.X\n",
    "        in_size = layers[0]\n",
    "        for i, l_size in enumerate(layers[1:]):\n",
    "            scale = (2./in_size)**.5\n",
    "            W_val = numpy.random.normal(scale=scale, size=(in_size, l_size))\n",
    "            W_val = W_val.astype(config.floatX)\n",
    "            W = shared(value=W_val)\n",
    "            b_val = numpy.zeros(shape=(l_size, )).astype(config.floatX)\n",
    "            b = shared(value=b_val)\n",
    "            \n",
    "            out = tensor.dot(out, W) + b\n",
    "            if i == len(layers)-2:\n",
    "                out = tensor.nnet.softmax(out)\n",
    "            else:\n",
    "                out = tensor.nnet.relu(out)\n",
    "            \n",
    "            self.parameters += [W, b]\n",
    "            in_size = l_size\n",
    "            \n",
    "        prediction = out\n",
    "        error = tensor.eq(tensor.argmax(prediction, axis=1), self.y).mean()\n",
    "        cost = tensor.nnet.categorical_crossentropy(prediction, self.y).mean()\n",
    "        \n",
    "        self.lr = shared(value=numpy.float32(0.))\n",
    "        updates = [(par, par - self.lr * gradient.grad(cost, par))\n",
    "                  for par in self.parameters]\n",
    "            \n",
    "        self.train_fn = function(\n",
    "            inputs=[self.X, self.y],\n",
    "            outputs=[cost, error],\n",
    "            updates=updates,\n",
    "            allow_input_downcast=True)\n",
    "        self.valid_fn = function(\n",
    "            inputs=[self.X, self.y],\n",
    "            outputs=[cost, error], allow_input_downcast=True)\n",
    "            \n",
    "        \n",
    "    def train(self, X, y, X_val, y_val, lr=.01, n_epochs=2000):\n",
    "        \n",
    "        self.lr.set_value(numpy.float32(lr))\n",
    "        scale = X.max(axis=0) - X.min(axis=0)\n",
    "        min_val = X.min(axis=0)\n",
    "        X_1 = (X + min_val)/scale\n",
    "        X_val_1 = (X_val + min_val)/scale\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            train_out = self.train_fn(X_1, y)\n",
    "            valid_out = self.valid_fn(X_val_1, y_val)\n",
    "#             print epoch, 'train', train_out\n",
    "#             print epoch, 'val  ', valid_out\n",
    "            \n",
    "            \n",
    "        return train_out[1], valid_out[1]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject left out:  0\n",
      "0.69696969697 0.333333333333\n",
      "subject left out:  1\n",
      "0.69696969697 0.333333333333\n",
      "subject left out:  2\n",
      "0.757575757576 1.0\n",
      "subject left out:  3\n",
      "0.787878787879 0.333333333333\n",
      "subject left out:  4\n",
      "0.666666666667 0.333333333333\n",
      "subject left out:  5\n",
      "0.666666666667 0.333333333333\n",
      "subject left out:  6\n",
      "0.666666666667 0.666666666667\n",
      "subject left out:  7\n",
      "0.757575757576 0.333333333333\n",
      "subject left out:  8\n",
      "0.636363636364 0.333333333333\n",
      "subject left out:  9\n",
      "0.69696969697 0.666666666667\n",
      "subject left out:  10\n",
      "0.545454545455 0.666666666667\n",
      "subject left out:  11\n",
      "0.636363636364 0.333333333333\n",
      "0.684343434343 0.472222222222\n"
     ]
    }
   ],
   "source": [
    "n_subjects = 12\n",
    "val_error_sum = 0\n",
    "train_error_sum = 0\n",
    "\n",
    "for left_out in range(n_subjects):\n",
    "    \n",
    "    print 'subject left out: ', left_out\n",
    "    valid_indices = [left_out, n_subjects+left_out, 2*n_subjects+left_out]\n",
    "    data_train = numpy.delete(d_all, valid_indices, axis=0)\n",
    "    data_valid = d_all[valid_indices, :]\n",
    "    target_train = numpy.delete(session_n, valid_indices, axis=0)\n",
    "    target_valid = session_n[valid_indices]\n",
    "    \n",
    "    mlp = MLP()\n",
    "    train_error, val_error = mlp.train(data_train, target_train, data_valid, target_valid)\n",
    "    print train_error, val_error\n",
    "    val_error_sum += val_error\n",
    "    train_error_sum += train_error\n",
    "    \n",
    "print train_error_sum/n_subjects, val_error_sum/n_subjects"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
